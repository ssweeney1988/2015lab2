{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## all imports\n",
    "from IPython.display import HTML\n",
    "import chardet\n",
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import requests\n",
    "import operator\n",
    "import socket\n",
    "import _pickle\n",
    "import re # regular expressions\n",
    "\n",
    "from pandas import Series\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "#from secret import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='docs.google.com', port=443): Max retries exceeded with url: /spreadsheets/d/1DSHrCzgdbTgasnsxzJEloAT5wnH3u0EH4ZKwnhLnkVU/pub?output=csv (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x11aa24588>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Users/samuels/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 138\u001b[0;31m                 (self.host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuels/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuels/anaconda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/Users/samuels/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, **response_kw)\u001b[0m\n\u001b[1;32m    593\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuels/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuels/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sock'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuels/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuels/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 147\u001b[0;31m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x11aa24588>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/Users/samuels/anaconda/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m                 )\n",
      "\u001b[0;32m/Users/samuels/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, **response_kw)\u001b[0m\n\u001b[1;32m    642\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 643\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuels/anaconda/lib/python3.6/site-packages/requests/packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='docs.google.com', port=443): Max retries exceeded with url: /spreadsheets/d/1DSHrCzgdbTgasnsxzJEloAT5wnH3u0EH4ZKwnhLnkVU/pub?output=csv (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x11aa24588>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6e04bb46000d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://docs.google.com/spreadsheets/d/1DSHrCzgdbTgasnsxzJEloAT5wnH3u0EH4ZKwnhLnkVU/pub?output=csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuels/anaconda/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuels/anaconda/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuels/anaconda/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    486\u001b[0m         }\n\u001b[1;32m    487\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuels/anaconda/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuels/anaconda/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mProxyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='docs.google.com', port=443): Max retries exceeded with url: /spreadsheets/d/1DSHrCzgdbTgasnsxzJEloAT5wnH3u0EH4ZKwnhLnkVU/pub?output=csv (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x11aa24588>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.get('https://docs.google.com/spreadsheets/d/1DSHrCzgdbTgasnsxzJEloAT5wnH3u0EH4ZKwnhLnkVU/pub?output=csv')\n",
    "data = r.content\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "#df = pd.read_csv('Q:\\\\Sam\\\\Untitled spreadsheet - Sheet1.csv')\n",
    "df = pd.read_csv(io.BytesIO(data))\n",
    "\n",
    "# , names=r_cols\n",
    "#df = df[1:]\n",
    "#print(ratings.count)\n",
    "\n",
    "df.head(5) \n",
    "#df.shape\n",
    "\n",
    "# to slice a data frame\n",
    "#sliced= df.ix[:5, :10]\n",
    "#sliced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[\"Day\"] =np.nan\n",
    "df[\"Month\"] =np.nan\n",
    "df[\"Year\"] =np.nan\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " #for index, row in df.iterrows():\n",
    "    #df.Date=df.Date.astype('str') \n",
    "    #print(df.Date)\n",
    "    #df[\"Date\"].str.split(\"-\")\n",
    "df[\"Day\"] = df.Date.str[:2]\n",
    "df[\"Month\"] = df.Date.str[3:6]\n",
    "df[\"Year\"] = str(20) + df.Date.str[-2:]\n",
    "df[\"City\"] = df[\"Venue\"]\n",
    "df[\"Station_Code\"] = df[\"City\"]\n",
    "\n",
    "\n",
    "df.head()\n",
    "    \n",
    "    #print(\"yes\")\n",
    "    #print row['Date'], row['Rain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df.Venue.count()\n",
    "#find all unique data\n",
    "#df.Venue.unique()\n",
    "len(df.Venue.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "di_cities= {\"MCG\": \"Melbourne\",\n",
    " \"Gabba\": \"Brisbane\",\n",
    " \"Etihad Stadium\": \"Melbourne\",\n",
    " \"Adelaide Oval\": \"Adelaide\",\n",
    " \"Manuka Oval\": \"Canberra\",\n",
    " \"Domain Stadium\": \"Perth\",\n",
    " \"Metricon Stadium\": \"Gold Coast\",\n",
    " \"Blundstone Arena\": \"Hobart\",\n",
    " \"SCG\": \"Sydney\",\n",
    " \"Spotless Stadium\": \"Homebush\",\n",
    " \"ANZ Stadium\": \"Homebush\",\n",
    " \"Simonds Stadium\": \"Geelong\",\n",
    " \"Aurora Stadium\": \"Launceston\",\n",
    " \"Cazaly’s Stadium\": \"Cairns\",\n",
    " \"TIO Stadium\": \"Darwin\",\n",
    " \"Traeger Park\": \"Alice Springs\",\n",
    " \"Westpac Stadium\": \"Wellington\",\n",
    " \"AAMI Stadium\": \"Adelaide\",\n",
    " \"Blacktown Park\": \"Homebush\" }\n",
    "\n",
    "df= df.replace({\"City\": di_cities})     \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (df.City)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "di_stations= {\n",
    "     \"Melbourne\": \"086338\",\n",
    "     \"Brisbane\": \"222\",\n",
    "     \"Melbourne\": \"86338\",\n",
    "     \"Adelaide\": \"xx\",\n",
    "     \"Canberra\": \"33\",\n",
    "     \"Perth\": \"xx\",\n",
    "     \"Gold Coast\": \"xx\",\n",
    "     \"Hobart\": \"xx\",\n",
    "     \"Sydney\": \"xx\",\n",
    "     \"Homebush\": \"xx\",\n",
    "     \"Homebush\": \"xx\",\n",
    "     \"Geelong\": \"xx\",\n",
    "     \"Launceston\": \"xx\",\n",
    "     \"Cairns\": \"xx\",\n",
    "     \"Darwin\": \"xx\",\n",
    "     \"Alice Springs\": \"xx\",\n",
    "     \"Wellington\": \"xx\",\n",
    "     \"Adelaide\": \"xx\",\n",
    "     \"Homebush\": \"xx\" }\n",
    "\n",
    "df= df.replace({\"Station_Code\": di_stations})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(di_stations[\"Melbourne\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.City.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rename days\n",
    "di_days= {\"01\": '0', \n",
    "     \"02\": '1',\n",
    "     \"03\": '2',\n",
    "     \"04\": '3',\n",
    "     \"05\": '4',\n",
    "     \"06\": '5',\n",
    "     \"07\": '6',\n",
    "     \"08\": '7',\n",
    "     \"09\": '8',\n",
    "     \"10\": '9',\n",
    "     \"11\": '10',\n",
    "     \"12\": '11',\n",
    "     \"13\": '12',\n",
    "     \"14\": '13',\n",
    "     \"15\": '14',\n",
    "     \"16\": '15',\n",
    "     \"17\": '16',\n",
    "     \"18\": '17',\n",
    "     \"19\": '18',\n",
    "     \"20\": '19',\n",
    "     \"21\": '20',\n",
    "     \"22\": '21',\n",
    "     \"23\": '22',\n",
    "     \"24\": '23',\n",
    "     \"25\": '24',\n",
    "     \"26\": '25',\n",
    "     \"27\": '26',\n",
    "     \"28\": '27',\n",
    "     \"29\": '29',\n",
    "     \"30\": '29',\n",
    "     \"31\": '30'}\n",
    "\n",
    "#df = df.replace({\"Day\": di_days})\n",
    "\n",
    "print(di_days[\"04\"])\n",
    "\n",
    "\n",
    "#http://stackoverflow.com/questions/20250771/remap-values-in-pandas-column-with-a-dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.Month.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "di_months= {\"Mar\": '2',\n",
    "     \"Apr\": '3', \n",
    "     \"May\": '4',\n",
    "     \"Jun\": '5',\n",
    "     \"Jul\": '6',\n",
    "     \"Aug\": '7',\n",
    "     \"Sep\": '8',\n",
    "     \"Oct\": '9'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "len(di_days.unique())\n",
    "# we may need to do a dictionary for months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#To access the key simple for loop, to access the value you need to call the di with the key\n",
    "for i in di:\n",
    "    print (i)\n",
    "for i in di:\n",
    "    print (di[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#move the last 5 columns to the front, last 5 columns + all colums not including the last 5\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-6:] + cols[:-6]\n",
    "df=df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (di_months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#result= df[(df['HomeTeam'] == \"Arsenal\") | (df.AwayTeam == \"Arsenal\")]\n",
    "Base_Link_01= \"http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_nccObsCode=136&p_display_type=dailyDataFile&p_startYear=\"\n",
    "Base_Link_02= \"&p_c=-1490869729&p_stn_num=\"\n",
    "\n",
    "# we probabley actually want while loop for each city, \n",
    "# do we even need the for loop, can we just do if witn an iff statement, because we dont have multiples across each row, we\n",
    "# either have the city or we dont, but maybe we need the for loop so it will keep running for each row and not\n",
    "# just stop when it finds what its after?\n",
    "#ITERATIVE OVER FOR EACH LOCATION INDIVIDUALLY- then we could start the loop again by running a loop over our location list\n",
    "#we might want to do a pandas sort where we find all rows where city and year are the same- so we just open a webpage once\n",
    "for index, row in df.iterrows():\n",
    "    #print(\"finding data\")\n",
    "    if row['City'] == \"Melbourne\":\n",
    "            if row['Year'] == \"2017\":\n",
    "                req = requests.get(Base_Link_01 + row['Year'] + Base_Link_02 + row['Station_Code'])\n",
    "                print (Base_Link_01 + row['Year'] + Base_Link_02 + row['Station_Code'])\n",
    "                soup = BeautifulSoup(req.text, 'html.parser')\n",
    "                #print(soup)\n",
    "        \n",
    "else:\n",
    "    #print (\"All data analysed\")\n",
    "    \n",
    "## lets start to scrape the details off this page\n",
    "#we know once we have written this once, we can then very easilly make the rest of this a function, not really sure how\n",
    "#to do it in a more efficient way, without all then nested loops- who knows, but functions will clean it up a LOT, so\n",
    "# we can write out a script once rather than 5 indivudal times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "req_01 = requests.get('http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_nccObsCode=136&p_display_type=dailyDataFile&p_startYear=2017&p_c=-1490869729&p_stn_num=86338')\n",
    "soup = BeautifulSoup(req_01.text, 'html.parser')\n",
    "#print (soup.prettify())\n",
    "\n",
    "<th scope=\"row\">\n",
    "    3rd\n",
    "   </th>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "we just go and do a python pandas filter \n",
    "by- city (pandas filter)\n",
    "by- year- (pandas filter) we can find what years come back for each city with a df.unique() then we can assign that to a list and then\n",
    "wecan  #df.Venue.unique()\n",
    "this will then allow us to just run a loop for all values once \n",
    "\n",
    "we can then initiate this \n",
    "\n",
    "-city\n",
    "- list of unique years\n",
    "- create the df with the city and years, day, month filter but groupby the year  (GROUP BY THE YEAR, filter all in)\n",
    "- open up website and run loop over it to get out the days and months then append the data to the rain part, show in filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df_city= df[df.City == \"Melbourne\"]\n",
    "    result =df[(df.City == \"Melbourne\") & (df.Year == Year_Array[count])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base_Link_01= \"http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_nccObsCode=136&p_display_type=dailyDataFile&p_startYear=\"\n",
    "Base_Link_02= \"&p_c=-1490869729&p_stn_num=\"\n",
    "\n",
    "City= \"Melbourne\"\n",
    "df_city= df[df.City == City]\n",
    "Year_Array= df_city.Year.unique()\n",
    "#print(len(Year_Array))\n",
    "count = 0\n",
    "\n",
    "for i in Year_Array:\n",
    "    result =df_city.loc[df.Year == Year_Array[count]]\n",
    "    req = requests.get(Base_Link_01 + Year_Array[count] + Base_Link_02 + di_stations[City])\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    print (Base_Link_01 + Year_Array[count] + Base_Link_02 + di_stations[City])\n",
    "    count= count + 1\n",
    "    for index, row in result.iterrows():\n",
    "        print (row['Day'])\n",
    "    \n",
    "    if count == len(Year_Array):\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Base_Link_01= \"http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_nccObsCode=136&p_display_type=dailyDataFile&p_startYear=\"\n",
    "Base_Link_02= \"&p_c=-1490870557&p_stn_num=\"\n",
    "\n",
    "City= \"Melbourne\"\n",
    "df_city= df[df.City == City]\n",
    "Year_Array= df_city.Year.unique()\n",
    "#print(len(Year_Array))\n",
    "count = 0\n",
    "\n",
    "for i in Year_Array:\n",
    "    result =df_city.loc[df.Year == Year_Array[count]]\n",
    "    req = requests.get(Base_Link_01 + Year_Array[count] + Base_Link_02 + di_stations[City])\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    tbody= soup.find('tbody')\n",
    "    print (Base_Link_01 + Year_Array[count] + Base_Link_02 + di_stations[City])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (df.City)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Base_Link_01= \"http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_nccObsCode=136&p_display_type=dailyDataFile&p_startYear=\"\n",
    "Base_Link_02= \"&p_c=-1490870976&p_stn_num=\"\n",
    "\n",
    "City= \"Melbourne\"\n",
    "df_city= df[df.City == City]\n",
    "Year_Array= df_city.Year.unique()\n",
    "#print(len(Year_Array))\n",
    "count = 0\n",
    "\n",
    "for i in Year_Array:\n",
    "    result =df_city.loc[df.Year == Year_Array[count]]\n",
    "    req = requests.get(Base_Link_01 + Year_Array[count] + Base_Link_02 + di_stations[City])\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    tbody= soup.find('tbody')\n",
    "    tr = tbody.find_all('tr', class_=lambda x: x != 'graphcell')\n",
    "    print (Base_Link_01 + \"2017\" + Base_Link_02 + di_stations[City])\n",
    "    count= count + 1\n",
    "    for index, row in result.iterrows():\n",
    "        print (row['Day'] + \" \" + row['Month'] + \" \" + row['Year'])\n",
    "        list = (tr[int(di_days[row['Day']])])\n",
    "        over = list.find_all('td')\n",
    "        result = float((over[int(di_months[row['Month']])]).string)\n",
    "        row[\"Rain\"] = result\n",
    "        print (row[\"Rain\"])\n",
    "    \n",
    "    if count == len(Year_Array):\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def foo(val1, val2, val3):\n",
    "    print(val1 + val2 + val3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "foo(1,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base_Link_01= \"http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_nccObsCode=136&p_display_type=dailyDataFile&p_startYear=\"\n",
    "Base_Link_02= \"&p_c=-1490870976&p_stn_num=\"\n",
    "\n",
    "City= \"Melbourne\"\n",
    "df_city= df[df.City == City]\n",
    "Year_Array= df_city.Year.unique()\n",
    "#print(len(Year_Array))\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i in Year_Array:\n",
    "    result =df_city.loc[df.Year == Year_Array[count]]\n",
    "    req = requests.get(Base_Link_01 + Year_Array[count] + Base_Link_02 + di_stations[City])\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    tbody= soup.find('tbody')\n",
    "    tr = tbody.find_all('tr', class_=lambda x: x != 'graphcell')\n",
    "    print (Base_Link_01 + Year_Array[count] + Base_Link_02 + di_stations[City])\n",
    "    count= count + 1\n",
    "    for index, row in result.iterrows():\n",
    "        print (row['Day'] + \" \" + row['Month'] + \" \" + row['Year'])\n",
    "        list = (tr[int(di_days[row['Day']])])\n",
    "        over = list.find_all('td')\n",
    "        result = float((over[int(di_months[row['Month']])]).string)\n",
    "        #return result\n",
    "        print (result)\n",
    "\n",
    "    \n",
    "    if count == len(Year_Array):\n",
    "        break\n",
    "\n",
    "\n",
    "#print (df.Rain)\n",
    "# return this as a function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# iterate through all the melbourne 2017's\n",
    "Days = '18'\n",
    "City= \"Melbourne\"\n",
    "df_city= df[df.City == City]\n",
    "#df_city.head()\n",
    "result =df_city.loc[df.Year == \"2016\"]\n",
    "#result.head(60)\n",
    "req = requests.get('http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_nccObsCode=136&p_display_type=dailyDataFile&p_startYear=2016&p_c=-1490870557&p_stn_num=086338')\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "tbody= soup.find('tbody')\n",
    "tr = tbody.find_all('tr', class_=lambda x: x != 'graphcell')\n",
    "#print (tr[1])\n",
    "# we now have a list of tr's, 0 tr is equal to date 1\n",
    "# if its one we want to enter list 0\n",
    "#print(di_days[\"04\"])\n",
    "list =  (tr[int(di_days[Days])])\n",
    "#print (list)\n",
    "over= list.find_all('td')\n",
    "print (over[2])\n",
    "\n",
    "\n",
    "#for index, row in result.iterrows():\n",
    "\n",
    "    #print (row['Day'])\n",
    "\n",
    "#th scope=\"row\">22nd</th><\n",
    "      #<tr>\n",
    "   #<th scope=\"row\">\n",
    "    #1st\n",
    "    \n",
    "    \n",
    "#we want to iterate over the result rows to find day, then we find date, \n",
    "# match up with the date- as we have changed it- but we could just do it as a dictionary- lets\n",
    "\n",
    "\n",
    "\n",
    "#run_01.head()\n",
    "#for index, row in run_01.iterrows():\n",
    "    #print (row['c1'], row['c2'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# do the scraping\n",
    "\n",
    "day = \"15th\"\n",
    "month = 4\n",
    "#Apr = # we want to do this as an iteration, so we might have to set up a loop number to go throguh to april\n",
    "\n",
    "\n",
    "req = requests.get('http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_nccObsCode=136&p_display_type=dailyDataFile&p_startYear=2017&p_c=-1490870141&p_stn_num=86338')\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "for index, row in result.iterrows():\n",
    "    #found the day\n",
    "    str(soup.find(\"table\", \"wikitable\"))\n",
    "        print (row['Day'])\n",
    "\n",
    "#<th scope=\"row\">\n",
    "    #3rd\n",
    "   #</th>\n",
    "\n",
    "\n",
    "#.find_all(\"tr\")]\n",
    "\n",
    "\n",
    "#find the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Day = 3\n",
    "new = Day -1\n",
    "print (Day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sort by year, sort by day, sort by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#venues list- assign venues to weather stations, include etihad but can get rid of that later\n",
    "#if we need to loop over for dates we may want to change the dates to be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "req = requests.get(\"http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_nccObsCode=136&p_display_type=dailyDataFile&p_startYear=2016&p_c=-1490870557&p_stn_num=086338\").text\n",
    "print (req)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.bom.gov.au/climate/data/\n",
    "\n",
    "ge the website link for each of them and substitute in the station name into the request as needed- see if its a consistent format across web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(soup.find_all(\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_url= \"http://www.football-data.co.uk/\" user_directory= \"Q:\\Sam\" \n",
    "for link in soup.find_all(\"a\"): \n",
    "    if \"Premier\" in link.text: \n",
    "        name= link.get(\"href\") \n",
    "        if name.endswith('csv'): \n",
    "            newname= str(\"EPL\" + name[8:-9] + \"_\" + name[10:-7] ) current_link=full_url + name download_path = user_directory + \"\\\" + new_name + \".csv\" output= open(download_path, 'wb') response= requests.get(current_link) output.write(response.content) print (\"downloading\" + \" \" + download_path) print (\"downloading\" + \" \" + current_link) output.close()\n",
    "    #print (new_name)\n",
    "    #print (current_link)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
